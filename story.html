<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Kernel: Introduce Multikernel Architecture Support</title>
    <style>
        body {
            font-family: Georgia, serif;
            font-size: 12pt;
            line-height: 1.4;
            margin: 20px;
            color: #000000;
        }
        h1 {
            font-size: 18pt;
        }
        .story-info {
            font-size: 10pt;
            color: #666666;
            margin: 10px 0;
        }
        .comment {
            border-left: 1px solid #cccccc;
            margin: 10px 0;
            padding-left: 10px;
        }
        .comment-header {
            font-size: 10pt;
            font-weight: bold;
            margin-bottom: 5px;
        }
        .comment-text {
            margin: 5px 0;
        }
        .comment-level-0 { margin-left: 0; }
        .comment-level-1 { margin-left: 20px; }
        .comment-level-2 { margin-left: 40px; }
        .comment-level-3 { margin-left: 60px; }
        .comment-level-4 { margin-left: 80px; }
        .comment-level-5 { margin-left: 100px; }
    </style>
</head>
<body>
<h1>Kernel: Introduce Multikernel Architecture Support</h1>
<div class="story-info">
  <p>Author: ahlCVA | 
  Points: 174 | 
  Posted: 2025-09-19T15:29:25.000Z</p>
</div>
<div class="comments-section">
<h2>Comments</h2>
<div class="comment comment-level-0">
  <div class="comment-header">
    andutu
  </div>
  <div class="comment-text">Pretty cool, sound similar to what Barrelfish OS enabled (<a href="https:&#x2F;&#x2F;barrelfish.org&#x2F;" rel="nofollow">https:&#x2F;&#x2F;barrelfish.org&#x2F;</a>).</div>
<div class="comment comment-level-1">
  <div class="comment-header">
    intermerda
  </div>
  <div class="comment-text">Tim Roscoe gave an interesting Keynote at OSDI &#x27;21 titled &quot;It&#x27;s Time for Operating Systems to Rediscover Hardware&quot; - <a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=36myc8wQhLo" rel="nofollow">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=36myc8wQhLo</a>. He was involved with the Barrelfish project.</div>
<div class="comment comment-level-2">
  <div class="comment-header">
    mensi
  </div>
  <div class="comment-text">That just reminded me that he&#x27;s always insisted on not being called Tim: <a href="https:&#x2F;&#x2F;people.inf.ethz.ch&#x2F;troscoe&#x2F;" rel="nofollow">https:&#x2F;&#x2F;people.inf.ethz.ch&#x2F;troscoe&#x2F;</a></div>
</div>
</div>
</div>
<div class="comment comment-level-0">
  <div class="comment-header">
    perching_aix
  </div>
  <div class="comment-text">Does this mean resiliency against kernel panics?</div>
<div class="comment comment-level-1">
  <div class="comment-header">
    sedatk
  </div>
  <div class="comment-text">&gt; - Improved fault isolation between different workloads</p><p>Yes.</div>
<div class="comment comment-level-2">
  <div class="comment-header">
    ATechGuy
  </div>
  <div class="comment-text">That&#x27;s what the author is claiming. Practically, VM-level strong fault isolation cannot be achieved without isolation support from the hardware aka virtualization.</div>
<div class="comment comment-level-3">
  <div class="comment-header">
    eqvinox
  </div>
  <div class="comment-text">Hardware without something like SR-IOV is straight up going to be unshareable for the foreseeable future; things like ring buffers would need a whole bunch of coordination between kernels to share.  SR-IOV (or equivalent) makes it workable, an IOMMU (or equivalent) then provides isolation.</div>
</div>
</div>
</div>
</div>
<div class="comment comment-level-0">
  <div class="comment-header">
    rwmj
  </div>
  <div class="comment-text">Sounds similar to CoLinux where you could run a &quot;cooperative Linux&quot; alongside Windows <a href="http:&#x2F;&#x2F;www.colinux.org&#x2F;" rel="nofollow">http:&#x2F;&#x2F;www.colinux.org&#x2F;</a></div>
<div class="comment comment-level-1">
  <div class="comment-header">
    brcmthrowaway
  </div>
  <div class="comment-text">This was underrated!</div>
</div>
<div class="comment comment-level-1">
  <div class="comment-header">
    joseph2024
  </div>
  <div class="comment-text">HP printers are similar. They run Linux on two cores and an RTOS on the other.</div>
<div class="comment comment-level-2">
  <div class="comment-header">
    lpribis
  </div>
  <div class="comment-text">Relatively common in more powerful embedded systems. See for example <a href="https:&#x2F;&#x2F;www.openampproject.org&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.openampproject.org&#x2F;</a> which is a framework for running multiple cores of Linux, baremtal, or an RTOS together.</div>
</div>
</div>
<div class="comment comment-level-1">
  <div class="comment-header">
    cout
  </div>
  <div class="comment-text">IIRC, colinux is similar to user mode Linux, which boots a kernel in userland. That is, the kernel runs under windows as an application rather than alongside it.</div>
<div class="comment comment-level-2">
  <div class="comment-header">
    rwmj
  </div>
  <div class="comment-text">It says on the web page I linked that:</p><p><i>Unlike in other Linux virtualization solutions such as User Mode Linux (or the forementioned VMware), special driver software on the host operating system is used to execute the coLinux kernel in a privileged mode (known as ring 0 or supervisor mode).</i></p><p><i>By constantly switching the machine&#x27;s state between the host OS state and and the coLinux kernel state, coLinux is given full control of the physical machine&#x27;s MMU (i.e, paging and protection) in its own specially allocated address space, and is able to act just like a native kernel, achieving almost the same performance and functionality that can be expected from a regular Linux which could have ran on the same machine standalone.</i></p><p>So my understanding is that it&#x27;s a Windows driver which contains a full Linux kernel and does some (scary sounding!)  time sharing with the Windows kernel running at the same CPL.</div>
</div>
</div>
</div>
<div class="comment comment-level-0">
  <div class="comment-header">
    vaastav
  </div>
  <div class="comment-text">How is this different from&#x2F;similar to Barrelfish?</div>
<div class="comment comment-level-1">
  <div class="comment-header">
    exe34
  </div>
  <div class="comment-text">mainline vs abandoned.</div>
</div>
</div>
<div class="comment comment-level-0">
  <div class="comment-header">
    zokier
  </div>
  <div class="comment-text">Interestingly the author has a startup revolving around this technology. Their webpage has some info: <a href="https:&#x2F;&#x2F;multikernel.io&#x2F;" rel="nofollow">https:&#x2F;&#x2F;multikernel.io&#x2F;</a></div>
<div class="comment comment-level-1">
  <div class="comment-header">
    sargun
  </div>
  <div class="comment-text">The author (Cong Wang) is building all sorts of neat stuff. Recently, they built kernelscript: <a href="https:&#x2F;&#x2F;github.com&#x2F;multikernel&#x2F;kernelscript" rel="nofollow">https:&#x2F;&#x2F;github.com&#x2F;multikernel&#x2F;kernelscript</a> -- another DSL for BPF that&#x27;s much more powerful than the C alternatives, without the complexity of C BPF. Previously, they were at Bytedance, so there&#x27;s a lot of hope that they understand the complexities of &quot;production&quot;.</div>
</div>
<div class="comment comment-level-1">
  <div class="comment-header">
    rurban
  </div>
  <div class="comment-text">I see. Even better than Xen, but needs much more memory than all the kvm instances. And as I heard memory is the real deal for mass hosters, not speed. So I am sceptical. I also don&#x27;t understand how it handles concurrent writes and states of shared hardware. Seems like a lot of overhead compared to kvm or Xen.</div>
</div>
</div>
<div class="comment comment-level-0">
  <div class="comment-header">
    9cb14c1ec0
  </div>
  <div class="comment-text">It would be interesting to see a detailed security assessment of this.  Would it provide security improvements over docker?</div>
<div class="comment comment-level-1">
  <div class="comment-header">
    eqvinox
  </div>
  <div class="comment-text">Docker is the wrong thing to compare against, especially considering it is an application and not a technology; the technology would be containerization. This competes against hardware virtualization support, if anything.</div>
</div>
<div class="comment comment-level-1">
  <div class="comment-header">
    esseph
  </div>
  <div class="comment-text">If you want some security improvements, move from docker to podman rootless + distroless containers.</p><p>If you need more security&#x2F;isolation, go to a VM or bare metal.</div>
</div>
</div>
<div class="comment comment-level-0">
  <div class="comment-header">
    messe
  </div>
  <div class="comment-text">Reminds me of exokernel architectures[0.5][1.5][2.5]. How is non-CPU resource multiplexing handled, or planned to be handled?</p><p>[0.5]: <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Exokernel" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Exokernel</a></p><p>[1.5]: <a href="https:&#x2F;&#x2F;wiki.osdev.org&#x2F;Exokernel" rel="nofollow">https:&#x2F;&#x2F;wiki.osdev.org&#x2F;Exokernel</a></p><p>[2.5]: &quot;Should array indices start at 0 or 1? My compromise of 0.5 was rejected without, I thought, proper consideration.&quot; — Stan Kelly-Bootle</div>
</div>
<div class="comment comment-level-0">
  <div class="comment-header">
    duendefm
  </div>
  <div class="comment-text">would this allow running both linux and bsd kernels?</div>
<div class="comment comment-level-1">
  <div class="comment-header">
    tremon
  </div>
  <div class="comment-text">It should be possible in theory, as long as both use the same communication interface. In practice, I think getting it to work on just one kernel is already a huge amount of work.</div>
<div class="comment comment-level-2">
  <div class="comment-header">
    viraptor
  </div>
  <div class="comment-text">It&#x27;s been done with more crazy setups already though: <a href="http:&#x2F;&#x2F;www.colinux.org&#x2F;" rel="nofollow">http:&#x2F;&#x2F;www.colinux.org&#x2F;</a> win+lin</div>
</div>
</div>
</div>
<div class="comment comment-level-0">
  <div class="comment-header">
    ch_123
  </div>
  <div class="comment-text">Reminds me of OpenVMS Galaxy on DEC Alpha systems, which allowed multiple instances of the OS to run side by side on the same hardware without virtualization.</p><p><a href="https:&#x2F;&#x2F;www.digiater.nl&#x2F;openvms&#x2F;doc&#x2F;alpha-v8.3&#x2F;83final&#x2F;aa_rezqe_te&#x2F;aa_rezqe_te.pdf" rel="nofollow">https:&#x2F;&#x2F;www.digiater.nl&#x2F;openvms&#x2F;doc&#x2F;alpha-v8.3&#x2F;83final&#x2F;aa_re...</a></div>
<div class="comment comment-level-1">
  <div class="comment-header">
    skissane
  </div>
  <div class="comment-text">IBM mainframes and Power servers have “partitions” (LPARs). My understanding of how they work, is they actually are software-based virtualisation, but the hypervisor is in the system firmware, not the OS. And some of the firmware is loaded from disk at boot-up, making it even closer to something like Xen-labelling it as “hardware” not “software” is more about marketing (and which internal teams own it within IBM) than than technical reality. Their mainframe partitioning system, PR&#x2F;SM, apparently began life as a stripped-down version of VM&#x2F;CMS, although I’m not sure how close the relationship between PR&#x2F;SM and z&#x2F;VM is in current releases.</p><p>This sounds like running multiple kernels in a shared security domain, which reduces the performance cost of transitions and sharing, but you lose the reliability and security advantages that a proper VM gives you. It reminds me of coLinux (essentially, a Linux kernel as a Windows NT device driver)</p><p>Does anyone have more details on how OpenVMS Galaxy was actually implemented? I believe it was available for both Alpha and Itanium, but not yet x86-64 (and probably never…)</div>
<div class="comment comment-level-2">
  <div class="comment-header">
    octotoad
  </div>
  <div class="comment-text">AFAIK, Galaxy was exclusive to Alpha, with no equivalent on Itanium, or any other platform.</div>
</div>
</div>
</div>
<div class="comment comment-level-0">
  <div class="comment-header">
    tremon
  </div>
  <div class="comment-text">&quot;while sharing the underlying hardware resources&quot;? At the risk of sounding too positive, my guess is that hell will freeze over before that will work reliably. Alternating access between the running kernels is probably the &quot;easy&quot; part (DMA and command queues solve a lot of this for free), but I&#x27;m thinking more of all the hardware that relies on state-keeping and serialization in the driver. There&#x27;s no way that e.g. the average usb or bluetooth vendor has &quot;multiple interleaved command sequences&quot; in their test setup.</p><p>I think Linux will have to move to a microkernel architecture before this can work. Once you have separate &quot;processes&quot; for hardware drivers, running two userlands side-by-side should be a piece of cookie (at least compared to the earlier task of converting the rest of the kernel).</p><p>Will be interesting to see where this goes. I like the idea, but if I were to go in that direction, I would choose something like a Genode kernel to supervise multiple Linux kernels.</div>
<div class="comment comment-level-1">
  <div class="comment-header">
    elteto
  </div>
  <div class="comment-text">You just don&#x27;t share certain devices, like Bluetooth. The &quot;main&quot; kernel will probably own the boot process and manage some devices exclusively. I think the real advantage is running certain applications isolated within a CPU subset, protected&#x2F;contained behind a dedicated kernel. You don&#x27;t have the slowdown of VMs, or have to fight against the isolation sieve that is docker.</div>
<div class="comment comment-level-2">
  <div class="comment-header">
    yjftsjthsd-h
  </div>
  <div class="comment-text">That&#x27;s fine for</p><p><pre><code>  - Enhanced security through kernel-level separation<br>  - Better resource utilization than traditional VM (KVM, Xen etc.)<br></code></pre><br>but I don&#x27;t think it works for</p><p><pre><code>  - Improved fault isolation between different workloads<br>  - Potential zero-down kernel update with KHO (Kernel Hand Over)<br></code></pre><br>since if the &quot;main&quot; kernel crashes or is supposed to get upgraded then you have to hand hardware back to it.</div>
<div class="comment comment-level-3">
  <div class="comment-header">
    raron
  </div>
  <div class="comment-text">&gt; since if the &quot;main&quot; kernel crashes or is supposed to get upgraded then you have to hand hardware back to it.</p><p>Isn&#x27;t that similar to starting up from hibernate to disk? Basically all of your peripherals are powered off and so probably can not keep their state.</p><p>Also you can actually stop a disk (member of a RAID device), remove the PCIe-SATA HBA card it is attached to, replace it with a different one, connect all back together without any user-space application noticing it.</div>
</div>
<div class="comment comment-level-3">
  <div class="comment-header">
    samus
  </div>
  <div class="comment-text">The old kernel boots the new kernel, possibly in a &quot;passive&quot; mode, performs a few sanity checks of the new instance, hands over control, and finally shuts itself down.</div>
</div>
</div>
</div>
<div class="comment comment-level-1">
  <div class="comment-header">
    vlovich123
  </div>
  <div class="comment-text">Is there anything that says that multiple kernels will be responsible for owning the drivers for HW? It could be that one kernel owns the hardware while the rest speak to the main kernel using a communication channel. That&#x27;s also presumably why KHO is a thing because you have to hand over when shutting down the kernel responsible for managing the driver.</div>
</div>
</div>
</div>
</body>
</html>